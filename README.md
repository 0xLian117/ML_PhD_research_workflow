# ML PhD Research Workflow

A Claude Code–driven, full-lifecycle research workflow for ML PhDs. Covers code development, experiment management, paper writing, literature review, and project management.

Designed as a **generic template**: clone, customize `CLAUDE.md` and `domain-template`, and you're ready to go.

---

## Core Philosophy

```
You = Research direction + Decisions       Claude = Execution + Quality guard
───────────────────────────────           ──────────────────────────────────
• Formulate hypotheses                     • Write code, run verification
• Choose approaches (A or B?)              • Review code / paper quality
• Approve plans                            • Maintain research journal
• Interpret results                        • Manage experiment configs & remote commands
```

**Key constraint**: Claude operates only on your local Mac (code development, review, writing). GPU training is executed manually on a remote server.

---

## Directory Structure

```
your_research/                          ← This repo (workflow + research assets)
│
├── context/                            ← Project context (loaded by /workon)
│   ├── project_a.md                    #   Full context for project A (purpose/arch/progress)
│   └── project_b.md                    #   Full context for project B
│
├── code/                               ← Code projects (each an independent git repo, untracked by this repo)
│   ├── project_a/                      #   git repo — your project A
│   ├── project_b/                      #   git repo — your project B
│   └── shared_lib/                     #   git repo — shared dependency (optional)
│
├── results/                            ← Experiment results downloaded from remote (analyzed locally)
│   ├── project_a/                      #   Organized as {method}_{YYYYMMDD_HHMMSS}/
│   └── project_b/
│
├── papers/                             ← Papers (local drafts → Overleaf)
│   ├── drafts/                         #   LaTeX drafts
│   ├── templates/                      #   Venue templates (ICML, NeurIPS, AAAI...)
│   └── published/                      #   Published (read-only protected)
│
├── slides/                             ← Beamer slides
│   └── templates/group_meeting.tex     #   Group meeting template (Metropolis theme)
│
├── reports/weekly/                     ← Weekly reports
├── literature/                         ← Reading notes
├── figures/                            ← Publication-quality figures (Python-generated)
├── explorations/                       ← Research sandbox (relaxed rules)
├── templates/                          ← General templates (weekly report/experiment/reading note)
│
├── quality_reports/                    ← Quality reports generated by Claude
│   ├── plans/                          #   Task plans
│   ├── session_logs/                   #   Research journal
│   └── reviews/                        #   Code/paper/experiment reviews
│
├── .claude/                            ← Claude Code configuration
│   ├── settings.json                   #   Permissions + hooks
│   ├── WORKFLOW_QUICK_REF.md           #   Quick reference
│   ├── hooks/ (4)                      #   Automation hooks
│   ├── rules/ (12+)                    #   3 always-on + 9 path-scoped
│   ├── agents/ (7)                     #   Read-only review agents
│   └── skills/ (14)                    #   Slash commands
│
├── CLAUDE.example.md                   ← Project context template (copy to CLAUDE.md and customize)
└── README.md                           ← This file

Project context lives in `context/` (not inside `code/`) to keep code repos clean.
Use `/workon [name]` to switch projects — automatically loads `context/{name}.md`.
```

---

## Quick Start

### 1. Clone & Customize

```bash
git clone https://github.com/0xLian117/ML_PhD_research_workflow.git my_research
cd my_research

# Create project context (required)
cp CLAUDE.example.md CLAUDE.md
# Edit CLAUDE.md: fill in your research topic, project registry, data interface, etc.

# Set up code projects under code/ (each is an independent repo)
mkdir -p code/
git clone <your-project-a> code/project_a
git clone <your-project-b> code/project_b

# Create context files for each project (used by /workon)
mkdir -p context/
# Write purpose, architecture, file structure, progress, etc. in context/project_a.md

# Create corresponding results directories
mkdir -p results/project_a results/project_b
```

### 2. Customize Domain Evaluation Criteria

Edit `.claude/rules/domain-template.md` and replace with your domain's evaluation metrics:

| Domain | Example Metrics |
|--------|----------------|
| Quantitative Finance | IC, IR, Sharpe |
| NLP | BLEU, ROUGE, Perplexity |
| CV | mAP, FID, SSIM |
| RL | Reward, Success Rate |

### 3. Customize Protected Files

Edit `.claude/hooks/protect-files.sh` to add critical files you don't want Claude to modify.

### 4. Remote Server (if GPU training is needed)

```bash
# Mirror the code/ structure on your remote GPU server
mkdir -p ~/my_research/code
cd ~/my_research/code
git clone <your-project-a> project_a
git clone <your-project-b> project_b
```

### 5. Launch Claude Code

```bash
cd my_research
claude
```

Claude automatically loads CLAUDE.md + all rules and discovers available skills.

---

## Execution Model

```
Local Mac (Claude operates here)                Remote GPU (you operate manually)
════════════════════════════════                ══════════════════════════════════
1. Develop code under code/
2. Claude reviews + local verification
3. cd code/project && git push  ──────────→    4. cd code/project && git pull
                                                5. python train.py --config ...
                                                6. tar -czf results.tar.gz outputs/
7. Download → extract to results/project/ ←────
8. /analyze-results results/project/run_xxx
```

**Key**: Each project under `code/` is an independent git repo with its own push/pull.
The workflow repo only tracks research assets (papers, notes, reports, plans) — not code or model weights.

---

## Daily Usage Guide

### Starting Your Day

```
You: /workon project_a
```

Claude automatically loads project context, checks git status, lists recent experiments, and outputs a structured briefing. You can then issue commands directly.

> Switch projects anytime with `/workon project_b`.

### Working Rhythm Within a Session

Claude has three always-on rules that run automatically — no manual triggering needed:

| Rule | Purpose | What You Do |
|------|---------|-------------|
| **plan-first** | Non-trivial tasks get a plan before execution | Approve or revise the plan |
| **orchestrator** | Auto code review, auto paper proofreading | Decide whether to fix when quality < 80 |
| **research-journal** | Logs progress to session log | Nothing (reminder every 20 responses) |

Typical interaction flow:

```
You: "Add gradient checkpointing to model.py"
Claude:
  1. [plan-first] Writes plan → waits for your approval
  2. [orchestrator] Implements → local verification (import OK) → code review (82/100)
  3. [research-journal] Records to session log
Claude: "Done. Score 82/100, ready to commit. Want to /commit?"
```

### Tips for Effective Collaboration with Claude

**Give clear intent, not step-by-step instructions**

```
# Good — state the goal
"Pretrain loss goes NaN at epoch 3, help me debug"
"Add a paragraph in the intro explaining why this problem matters"
"This experiment's IC is only 0.01, help me analyze possible causes"

# Bad — micromanaging
"Open model.py, find line 45, change 0.001 to 0.0001"
```

**Use slash commands for standard workflows**

Slash commands encapsulate complete workflows — more reliable and reproducible than free-form conversation:

```
/debug "loss NaN at epoch 3"              ← Better than "help me figure out why NaN"
/experiment "increase d_model from 128 to 256"  ← Auto-generates config + remote commands
/review-code code/project/model.py              ← 8-dimensional scoring, nothing missed
```

**Use `explorations/` for exploratory work**

Not sure if something will work? Want to quickly test an idea? Work directly in explorations/:

```
You: "Try replacing MSE loss with cosine similarity in explorations/"
```

explorations/ has relaxed rules: 60 points is enough, no plan required. If results are promising, "graduate" to the main code directory.

---

## Skills (Slash Commands)

Type the following commands in Claude Code to trigger the corresponding workflow.

### Code & Experiments

| Command | Purpose | Example |
|---------|---------|---------|
| `/workon [name]` | Switch to project context (loads docs + git status) | `/workon delta_learn` |
| `/commit` | Git commit (auto review + excludes large files/secrets) | `/commit` |
| `/review-code [file]` | Code review (8-dimensional scoring) | `/review-code code/my_project/model.py` |
| `/experiment [hypo]` | Create experiment: hypothesis → config → remote commands | `/experiment "d_model 128→256"` |
| `/analyze-results [dir]` | Analyze downloaded experiment results | `/analyze-results results/run_20250211` |
| `/compare [dirs]` | Compare multiple experiment results | `/compare results/baseline results/ablation` |
| `/data-check [target]` | Data pipeline check (NaN/bias/distribution) | `/data-check code/my_project/data/` |
| `/debug [symptom]` | Structured ML debugging (symptom → diagnosis → report) | `/debug "loss NaN at epoch 3"` |

### Academic Writing

| Command | Purpose | Example |
|---------|---------|---------|
| `/paper-draft [section]` | Draft paper section (LaTeX) | `/paper-draft method` |
| `/proofread [file]` | Proofread (grammar/spelling/LaTeX/terminology) | `/proofread papers/drafts/intro.tex` |
| `/lit-review [topic]` | Literature search + survey | `/lit-review "transformer time series"` |

### Reports & Presentations

| Command | Purpose | Example |
|---------|---------|---------|
| `/weekly-report` | Generate weekly research progress report | `/weekly-report` |
| `/slides [topic]` | Create Beamer group meeting slides | `/slides "this week's experiments"` |

### Research

| Command | Purpose | Example |
|---------|---------|---------|
| `/research-idea [desc]` | Formalize a vague idea into a research plan | `/research-idea "use contrastive learning to improve representations"` |

---

## Typical Workflows

### New Experiment (Full Loop)

```
1. /workon my_project              → Load project context
2. /experiment "hypothesis: X causes Y"  → Generate config + experiment record + remote commands
3. /review-code code/.../train.py  → Review code quality (optional)
4. /commit                         → Commit code (auto review, ≥80 to pass)
   └── (remote) git pull → train → tar results → download to results/
5. /analyze-results results/...    → Auto analysis report
6. /compare results/old results/new → Compare against baseline (optional)
```

### Debugging a Failed Experiment

```
1. /workon my_project
2. /debug "loss NaN at epoch 3"    → Structured investigation
   └── Auto: classify symptom → read code/config/logs → run checklist → diagnosis report
3. (Fix per report) → /commit → re-run on remote
```

### Writing a Paper

```
1. /lit-review "topic"               → Literature search + reading notes
2. /paper-draft intro                → Draft Introduction
3. /proofread papers/drafts/.../intro.tex  → Proofread
4. Copy to Overleaf                  → Final typesetting
```

### Weekly Routine

```
1. /weekly-report                   → Auto-summarize weekly progress (from git log + session logs)
2. /slides "this week's progress"   → Generate group meeting slides (optional)
```

### Quick Exploration

```
1. "Try XXX in explorations/"       → No plan needed, 60 points is enough
2. Good results → move to main code directory (graduate)
3. Bad results → keep the record, move to the next idea
```

---

## Automation System

### Rules (Auto-Active)

#### Always-On (3 rules)

| Rule | Purpose |
|------|---------|
| **plan-first** | Non-trivial tasks get a plan before execution, saved to `quality_reports/plans/` |
| **orchestrator** | Autonomous execution loop: CODE mode (implement → verify → review) / WRITE mode (draft → proofread → review), quality ≥ 80 to commit |
| **research-journal** | Research journal: three trigger points — after plan approval, upon discoveries, before session end |

#### Path-Scoped (9 rules, auto-activated by file path)

When Claude reads or edits files matching a path, the corresponding rule activates:

| Rule | Trigger Path | Focus |
|------|-------------|-------|
| **python-ml** | `**/*.py` | Numerical safety (nanmean/log1p/div-by-zero), reproducibility (seed), logging |
| **data-pipeline** | `**/data/**` | **Zero tolerance for look-ahead bias**, NaN handling, no train/val/test overlap |
| **model-conventions** | `**/model*/**` | Config-driven, forward shape annotations, gradient clipping |
| **experiment-protocol** | `**/config/**, **/scripts/**` | YAML versioning, seed=42, change one variable at a time |
| **academic-writing** | `**/*.tex, **/papers/**` | Consistent terminology, `\newcommand`, proper citations |
| **figures-tables** | `**/figures/**, **/plots/**` | 300 DPI, vector format, colorblind-friendly, ≥8pt font |
| **literature-protocol** | `**/literature/**` | Reading note format, BibTeX management |
| **exploration-fast-track** | `explorations/**` | Quality threshold lowered to 60, no plan required |
| **domain-template** | (customizable path) | Domain evaluation criteria — default: quantitative finance, replaceable |

### Agents (Review Agents)

All agents are **read-only** reviewers that generate reports to `quality_reports/reviews/` without modifying files.

| Agent | Function | Review Dimensions |
|-------|----------|-------------------|
| **code-reviewer** | Python/PyTorch code review | Structure, numerical safety, PyTorch correctness, reproducibility, memory, types, docs, style (8 dimensions) |
| **paper-reviewer** | Academic paper review (Reviewer 2 perspective) | Structure, contribution, method, experiments, writing, rejection prediction (6 dimensions) |
| **proofreader** | Text proofreading | Grammar, spelling, LaTeX, terminology consistency, academic tone |
| **experiment-reviewer** | Experiment design review | Hypothesis clarity, controlled variables, baseline validity, statistical rigor (5 dimensions) |
| **math-reviewer** | Math derivation/proof review | Logical correctness, symbol consistency, boundary conditions, completeness |
| **verifier** | End-to-end verification (only agent that can execute code) | Python import, config validity, LaTeX compilation |
| **literature-assistant** | Literature research assistant | Literature search, note organization, Related Work drafting |

### Hooks (Automation Hooks)

| Hook | Trigger | Purpose |
|------|---------|---------|
| **notify.sh** | Any notification | macOS desktop notification (osascript) |
| **protect-files.sh** | Before Edit/Write | Block modifications to protected files (settings.json, published papers, etc.) |
| **pre-compact.sh** | Before context compression | Save current state to session log (prevents context loss) |
| **log-reminder.py** | When Claude stops responding | Reminder if journal hasn't been updated in 20 responses |

---

## Session Management

### Context Protection for Long Sessions

Claude Code has a context window limit. When conversations get too long, it auto-compresses (compacts). This workflow's protection mechanisms:

1. **pre-compact hook** — Automatically writes current git status and latest plan to session log before compression
2. **plan-first rule** — Plans are saved to files (`quality_reports/plans/`), recoverable after compact
3. **research-journal rule** — Key findings and decisions are written to the journal in real-time, never lost

**If Claude seems to "forget" context after compact**, you can:
```
"Read the latest plan and session log to restore context"
```

### Session Recovery (Resuming in a New Session)

To resume previous work in a new Claude session:

```
You: /workon project_a
You: "Read the latest plan and session log, continue where we left off"
```

Claude will automatically:
1. Read CLAUDE.md (project overview)
2. Read context/project_a.md (project context)
3. Read latest plan + session log
4. Summarize current progress and confirm with you

---

## Quality Gates

| Scenario | Threshold | Details |
|----------|-----------|---------|
| Code commit (`/commit`) | ≥ 80/100 | code-reviewer auto-reviews .py files |
| Paper section | ≥ 80/100 | paper-reviewer + proofreader |
| Exploratory code (`explorations/`) | ≥ 60/100 | Relaxed standard, plan can be skipped |

**Scoring Dimensions**:
- Code: Correctness (30) + Numerical Safety (20) + Reproducibility (15) + Code Quality (15) + Documentation (10) + Style (10)
- Writing: Content (30) + Structure (20) + Clarity (20) + Formatting (15) + Language (15)

---

## Non-Negotiable Rules

1. **Zero tolerance for look-ahead bias** — No future information in data pipelines
2. **Every experiment must have a seed** — Default 42, ablation seeds [42, 123, 456, 789, 1024]
3. **NaN must be checked and handled** — Validate after every operation
4. **Test split disabled during development** — Only used for final evaluation
5. **Paper data must be reproducible** — Generated by code, not manually edited
6. **Code and workflow are separated** — Each project under code/ is an independent repo; the workflow repo does not track code

---

## Templates

| Template | Path | Purpose |
|----------|------|---------|
| Weekly Report | `templates/weekly_report.md` | Output format for `/weekly-report` |
| Experiment Log | `templates/experiment_log.md` | Experiment documentation format for `/experiment` |
| Reading Note | `templates/reading_note.md` | Literature reading note format |
| Group Meeting Slides | `slides/templates/group_meeting.tex` | Beamer template (Metropolis theme) |

---

## Customization Guide

| File to Customize | Description |
|-------------------|-------------|
| `CLAUDE.md` | **Required** — Your research topic, project registry, data interface, evaluation criteria |
| `context/*.md` | **Required** — Full context for each project (purpose/architecture/progress), used by `/workon` |
| `.claude/rules/domain-template.md` | Domain evaluation metrics (default: quantitative finance, replace with yours) |
| `.claude/hooks/protect-files.sh` | Protected file list |
| `.gitignore` | Project directory names under `code/` |
| `results/` | Create corresponding subdirectories for your projects |

---

## FAQ

**Q: Can Claude run GPU training directly?**
No. Claude only operates on your local Mac. `/experiment` generates remote run commands — copy them to your GPU server to execute.

**Q: What if `/commit` review doesn't pass?**
A score < 80 blocks the commit and lists the issues. Fix them and re-run `/commit`, or type "skip review" to force through (this will be noted in the commit message).

**Q: How do I protect critical files from being modified by Claude?**
Edit `.claude/hooks/protect-files.sh` and add files to `PROTECTED_BASENAMES` or path patterns.

**Q: What if I want to use code from explorations/ in production?**
"Graduation" process: raise quality score to ≥ 80 → move to the main code directory → mark "graduated to [path]" in the exploration README.

**Q: How do I resume previous work in a new session?**
`/workon [project]` → "Read the latest plan and session log." The plan-first and research-journal rules ensure key information is persisted in files.

**Q: Claude forgot what it was doing after context compact?**
The pre-compact hook automatically saves state. Tell Claude "Read the latest session log to restore context."
